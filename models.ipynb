{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139bbc25-87eb-4e90-9143-fa2c06326360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 21:59:38 WARN Utils: Your hostname, Abdelmajid-Macs-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.11.108 instead (on interface en0)\n",
      "24/05/04 21:59:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/04 21:59:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/04 21:59:48 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:00:01 WARN DAGScheduler: Broadcasting large task binary with size 1296.4 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7506495282373855\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis with Naive Bayes\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'twitter_training.csv'\n",
    "df = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Define column names\n",
    "df = df.withColumnRenamed('_c0', 'ID') \\\n",
    "       .withColumnRenamed('_c1', 'Topic') \\\n",
    "       .withColumnRenamed('_c2', 'Sentiment') \\\n",
    "       .withColumnRenamed('_c3', 'Text')\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare features and label\n",
    "tokenizer = RegexTokenizer(inputCol='Text', outputCol='words', pattern='\\\\W')\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "vectorizer = CountVectorizer(inputCol='filtered_words', outputCol='features')\n",
    "indexer = StringIndexer(inputCol='Sentiment', outputCol='label')\n",
    "\n",
    "# Split data into train and test sets\n",
    "(train_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create a Naive Bayes classifier\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='label', smoothing=1.0)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, vectorizer, indexer, nb])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbe978a-4b05-4a7d-9d74-3b408bbcc641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 22:00:12 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:00:23 WARN DAGScheduler: Broadcasting large task binary with size 1023.3 KiB\n",
      "24/05/04 22:00:24 WARN MemoryStore: Not enough space to cache rdd_51_2 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:24 WARN MemoryStore: Not enough space to cache rdd_51_1 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:24 WARN MemoryStore: Not enough space to cache rdd_51_0 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:24 WARN BlockManager: Persisting block rdd_51_0 to disk instead.\n",
      "24/05/04 22:00:24 WARN BlockManager: Persisting block rdd_51_1 to disk instead.\n",
      "24/05/04 22:00:24 WARN BlockManager: Persisting block rdd_51_2 to disk instead.\n",
      "24/05/04 22:00:28 WARN MemoryStore: Not enough space to cache rdd_51_2 in memory! (computed 246.1 MiB so far)\n",
      "24/05/04 22:00:33 WARN MemoryStore: Not enough space to cache rdd_51_1 in memory! (computed 246.1 MiB so far)\n",
      "24/05/04 22:00:33 WARN MemoryStore: Not enough space to cache rdd_51_0 in memory! (computed 246.1 MiB so far)\n",
      "24/05/04 22:00:38 WARN DAGScheduler: Broadcasting large task binary with size 1024.1 KiB\n",
      "24/05/04 22:00:38 WARN MemoryStore: Not enough space to cache rdd_51_2 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:38 WARN MemoryStore: Not enough space to cache rdd_51_1 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:38 WARN MemoryStore: Not enough space to cache rdd_51_0 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:42 WARN DAGScheduler: Broadcasting large task binary with size 1024.7 KiB\n",
      "24/05/04 22:00:43 WARN MemoryStore: Not enough space to cache rdd_51_1 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:43 WARN MemoryStore: Not enough space to cache rdd_51_2 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:43 WARN MemoryStore: Not enough space to cache rdd_51_0 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:47 WARN DAGScheduler: Broadcasting large task binary with size 1025.9 KiB\n",
      "24/05/04 22:00:47 WARN MemoryStore: Not enough space to cache rdd_51_2 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:47 WARN MemoryStore: Not enough space to cache rdd_51_0 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:47 WARN MemoryStore: Not enough space to cache rdd_51_1 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:51 WARN DAGScheduler: Broadcasting large task binary with size 1027.7 KiB\n",
      "24/05/04 22:00:52 WARN MemoryStore: Not enough space to cache rdd_51_1 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:52 WARN MemoryStore: Not enough space to cache rdd_51_2 in memory! (computed 106.4 MiB so far)\n",
      "24/05/04 22:00:52 WARN MemoryStore: Not enough space to cache rdd_51_0 in memory! (computed 106.4 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3674962395733625\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis with Decision Tree\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'twitter_training.csv'\n",
    "df = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Define column names\n",
    "df = df.withColumnRenamed('_c0', 'ID') \\\n",
    "       .withColumnRenamed('_c1', 'Topic') \\\n",
    "       .withColumnRenamed('_c2', 'Sentiment') \\\n",
    "       .withColumnRenamed('_c3', 'Text')\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare features and label\n",
    "tokenizer = RegexTokenizer(inputCol='Text', outputCol='words', pattern='\\\\W')\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "vectorizer = CountVectorizer(inputCol='filtered_words', outputCol='features')\n",
    "indexer = StringIndexer(inputCol='Sentiment', outputCol='label')\n",
    "\n",
    "# Split data into train and test sets\n",
    "(train_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label', maxDepth=5)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, vectorizer, indexer, dt])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099c0da5-b792-48ec-8c81-e36109216547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 22:01:03 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:01:07 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/05/04 22:01:07 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "24/05/04 22:01:21 WARN DAGScheduler: Broadcasting large task binary with size 1301.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8372760836865856\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import joblib\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'twitter_training.csv'\n",
    "df = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Define column names\n",
    "df = df.withColumnRenamed('_c0', 'ID') \\\n",
    "       .withColumnRenamed('_c1', 'Topic') \\\n",
    "       .withColumnRenamed('_c2', 'Sentiment') \\\n",
    "       .withColumnRenamed('_c3', 'Text')\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare features and label\n",
    "tokenizer = RegexTokenizer(inputCol='Text', outputCol='words', pattern='\\\\W')\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "vectorizer = CountVectorizer(inputCol='filtered_words', outputCol='features')\n",
    "indexer = StringIndexer(inputCol='Sentiment', outputCol='label')\n",
    "\n",
    "# Split data into train and test sets\n",
    "(train_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, vectorizer, indexer, lr])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a433661-54e8-4c14-aaa6-e29f5259e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 22:03:31 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:03:48 WARN DAGScheduler: Broadcasting large task binary with size 1301.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8372760836865856\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'twitter_training.csv'\n",
    "df = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Define column names\n",
    "df = df.withColumnRenamed('_c0', 'ID') \\\n",
    "       .withColumnRenamed('_c1', 'Topic') \\\n",
    "       .withColumnRenamed('_c2', 'Sentiment') \\\n",
    "       .withColumnRenamed('_c3', 'Text')\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare features and label\n",
    "tokenizer = RegexTokenizer(inputCol='Text', outputCol='words', pattern='\\\\W')\n",
    "# Keep the ID column by using a custom function to remove stop words\n",
    "stop_words = StopWordsRemover.loadDefaultStopWords('english')\n",
    "stop_words_remover = StopWordsRemover(inputCol='words', outputCol='filtered_words', stopWords=stop_words)\n",
    "vectorizer = CountVectorizer(inputCol='filtered_words', outputCol='features')\n",
    "indexer = StringIndexer(inputCol='Sentiment', outputCol='label')\n",
    "\n",
    "# Split data into train and test sets\n",
    "(train_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, vectorizer, indexer, lr])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d80b5e2-2d7c-4868-b23e-458dd1e19305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 22:05:07 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:05:24 WARN DAGScheduler: Broadcasting large task binary with size 1301.5 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8441132230274853\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'twitter_training.csv'\n",
    "df = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Define column names\n",
    "df = df.withColumnRenamed('_c0', 'ID') \\\n",
    "       .withColumnRenamed('_c1', 'Topic') \\\n",
    "       .withColumnRenamed('_c2', 'Sentiment') \\\n",
    "       .withColumnRenamed('_c3', 'Text')\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare features and label\n",
    "tokenizer = RegexTokenizer(inputCol='Text', outputCol='words', pattern='\\\\W')\n",
    "stop_words = StopWordsRemover.loadDefaultStopWords('english')\n",
    "stop_words_remover = StopWordsRemover(inputCol='words', outputCol='filtered_words', stopWords=stop_words)\n",
    "vectorizer = CountVectorizer(inputCol='filtered_words', outputCol='features')\n",
    "indexer = StringIndexer(inputCol='Sentiment', outputCol='label')\n",
    "\n",
    "# Select columns for processing (excluding 'ID')\n",
    "selected_cols = ['Text', 'Sentiment']\n",
    "df_selected = df.select(selected_cols)\n",
    "\n",
    "# Split data into train and test sets\n",
    "(train_data, test_data) = df_selected.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, vectorizer, indexer, lr])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c8e9e2-f8f8-4bed-ab40-7cf1cedfc59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 22:05:52 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:05:53 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:05:53 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:05:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/05/04 22:05:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/05/04 22:05:58 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/05/04 22:06:43 WARN DAGScheduler: Broadcasting large task binary with size 1034.3 KiB\n",
      "24/05/04 22:06:45 WARN DAGScheduler: Broadcasting large task binary with size 9.2 MiB\n",
      "24/05/04 22:06:46 WARN MemoryStore: Not enough space to cache rdd_56_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:06:46 WARN MemoryStore: Not enough space to cache rdd_56_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:06:46 WARN MemoryStore: Not enough space to cache rdd_56_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:06:46 WARN BlockManager: Persisting block rdd_56_0 to disk instead.\n",
      "24/05/04 22:06:46 WARN BlockManager: Persisting block rdd_56_2 to disk instead.\n",
      "24/05/04 22:06:46 WARN BlockManager: Persisting block rdd_56_1 to disk instead.\n",
      "24/05/04 22:07:25 WARN MemoryStore: Not enough space to cache rdd_56_2 in memory! (computed 272.9 MiB so far)\n",
      "24/05/04 22:08:05 WARN MemoryStore: Not enough space to cache rdd_56_1 in memory! (computed 272.9 MiB so far)\n",
      "24/05/04 22:08:11 WARN MemoryStore: Not enough space to cache rdd_56_0 in memory! (computed 272.9 MiB so far)\n",
      "24/05/04 22:08:36 WARN DAGScheduler: Broadcasting large task binary with size 9.2 MiB\n",
      "24/05/04 22:08:37 WARN MemoryStore: Not enough space to cache rdd_56_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:08:37 WARN MemoryStore: Not enough space to cache rdd_56_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:08:37 WARN MemoryStore: Not enough space to cache rdd_56_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:09:10 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "24/05/04 22:09:11 WARN MemoryStore: Not enough space to cache rdd_56_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:09:11 WARN MemoryStore: Not enough space to cache rdd_56_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:09:11 WARN MemoryStore: Not enough space to cache rdd_56_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:09:48 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "24/05/04 22:09:49 WARN MemoryStore: Not enough space to cache rdd_56_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:09:49 WARN MemoryStore: Not enough space to cache rdd_56_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:09:49 WARN MemoryStore: Not enough space to cache rdd_56_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:10:26 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "24/05/04 22:10:27 WARN MemoryStore: Not enough space to cache rdd_56_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:10:27 WARN MemoryStore: Not enough space to cache rdd_56_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:10:27 WARN MemoryStore: Not enough space to cache rdd_56_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:11:05 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:11:05 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "24/05/04 22:11:06 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:11:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/05/04 22:11:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/05/04 22:11:09 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/05/04 22:11:54 WARN DAGScheduler: Broadcasting large task binary with size 1034.3 KiB\n",
      "24/05/04 22:11:55 WARN DAGScheduler: Broadcasting large task binary with size 9.2 MiB\n",
      "24/05/04 22:11:56 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:11:56 WARN BlockManager: Persisting block rdd_136_2 to disk instead.\n",
      "24/05/04 22:11:56 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:11:56 WARN BlockManager: Persisting block rdd_136_1 to disk instead.\n",
      "24/05/04 22:11:56 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:11:56 WARN BlockManager: Persisting block rdd_136_0 to disk instead.\n",
      "24/05/04 22:12:34 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 272.9 MiB so far)\n",
      "24/05/04 22:13:13 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 272.9 MiB so far)\n",
      "24/05/04 22:13:19 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 272.9 MiB so far)\n",
      "24/05/04 22:13:46 WARN DAGScheduler: Broadcasting large task binary with size 9.2 MiB\n",
      "24/05/04 22:13:46 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:13:46 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:13:46 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:14:20 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "24/05/04 22:14:20 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:14:20 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:14:20 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:14:57 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "24/05/04 22:14:58 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:14:58 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:14:58 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:15:34 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "24/05/04 22:15:34 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:15:34 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:15:34 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:16:13 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "24/05/04 22:16:13 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:16:13 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:16:13 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:16:51 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "24/05/04 22:16:52 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:16:52 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:16:52 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:17:30 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "24/05/04 22:17:31 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:17:31 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:17:31 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:18:08 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "24/05/04 22:18:09 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:18:09 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:18:09 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:18:47 WARN DAGScheduler: Broadcasting large task binary with size 9.4 MiB\n",
      "24/05/04 22:18:47 WARN MemoryStore: Not enough space to cache rdd_136_2 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:18:47 WARN MemoryStore: Not enough space to cache rdd_136_0 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:18:47 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 113.0 MiB so far)\n",
      "24/05/04 22:19:25 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:19:25 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "24/05/04 22:19:26 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:19:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/05/04 22:19:28 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/05/04 22:19:29 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "24/05/04 22:20:15 WARN DAGScheduler: Broadcasting large task binary with size 1034.3 KiB\n",
      "24/05/04 22:20:16 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "24/05/04 22:20:17 WARN MemoryStore: Not enough space to cache rdd_226_2 in memory! (computed 113.1 MiB so far)\n",
      "24/05/04 22:20:17 WARN BlockManager: Persisting block rdd_226_2 to disk instead.\n",
      "24/05/04 22:20:17 WARN MemoryStore: Not enough space to cache rdd_226_1 in memory! (computed 113.1 MiB so far)\n",
      "24/05/04 22:20:17 WARN BlockManager: Persisting block rdd_226_1 to disk instead.\n",
      "24/05/04 22:20:17 WARN MemoryStore: Not enough space to cache rdd_226_0 in memory! (computed 113.1 MiB so far)\n",
      "24/05/04 22:20:17 WARN BlockManager: Persisting block rdd_226_0 to disk instead.\n",
      "24/05/04 22:20:54 WARN MemoryStore: Not enough space to cache rdd_226_2 in memory! (computed 272.9 MiB so far)\n",
      "24/05/04 22:21:32 WARN MemoryStore: Not enough space to cache rdd_226_1 in memory! (computed 272.9 MiB so far)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer, NGram, HashingTF, IDF\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis with RandomForest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'twitter_training.csv'\n",
    "df = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Define column names\n",
    "df = df.withColumnRenamed('_c0', 'ID') \\\n",
    "       .withColumnRenamed('_c1', 'Topic') \\\n",
    "       .withColumnRenamed('_c2', 'Sentiment') \\\n",
    "       .withColumnRenamed('_c3', 'Text')\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare features and label\n",
    "tokenizer = RegexTokenizer(inputCol='Text', outputCol='words', pattern='\\\\W')\n",
    "stop_words_remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "indexer = StringIndexer(inputCol='Sentiment', outputCol='label')\n",
    "\n",
    "# Add NGram feature extraction\n",
    "ngram = NGram(n=2, inputCol='filtered_words', outputCol='ngrams')\n",
    "hashingTF = HashingTF(inputCol='ngrams', outputCol='rawFeatures')\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "\n",
    "# Change to Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, indexer, ngram, hashingTF, idf, rf])\n",
    "\n",
    "# Split data into train and test sets\n",
    "(train_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Set up parameter grid and cross-validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [50, 100, 200])\n",
    "             .addGrid(rf.maxDepth, [5, 10])\n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "# Train the model\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = cvModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ea997b-e892-48f6-b796-a10147a71494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 22:23:03 WARN Utils: Your hostname, Abdelmajid-Macs-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.11.108 instead (on interface en0)\n",
      "24/05/04 22:23:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/04 22:23:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/04 22:23:12 WARN StopWordsRemover: Default locale set was [en_MA]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/04 22:23:24 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/05/04 22:23:24 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "24/05/04 22:23:38 WARN DAGScheduler: Broadcasting large task binary with size 1300.7 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8495145631067961\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer, RegexTokenizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Analysis with Logistic Regression\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'twitter_training.csv'\n",
    "df = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Define column names and drop the ID column\n",
    "df = df.withColumnRenamed('_c0', 'ID') \\\n",
    "       .withColumnRenamed('_c1', 'Topic') \\\n",
    "       .withColumnRenamed('_c2', 'Sentiment') \\\n",
    "       .withColumnRenamed('_c3', 'Text') \\\n",
    "       .drop('ID')  # Drop the ID column\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare features and label\n",
    "tokenizer = RegexTokenizer(inputCol='Text', outputCol='words', pattern='\\\\W')\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "vectorizer = CountVectorizer(inputCol='filtered_words', outputCol='features')\n",
    "indexer = StringIndexer(inputCol='Sentiment', outputCol='label')\n",
    "\n",
    "# Split data into train and test sets\n",
    "(train_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, vectorizer, indexer, lr])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6844f-453f-4b5d-b104-128f01404d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
